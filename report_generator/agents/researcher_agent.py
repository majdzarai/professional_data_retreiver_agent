#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Researcher Agent for the Report Generator

This module contains the Researcher Agent class that is responsible for executing
RAG queries and collecting evidence for each question in a report section.
"""

import json
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import subprocess

from ..utils.config import ReportGeneratorConfig

logger = logging.getLogger(__name__)


class ResearcherAgent:
    """Researcher Agent for the Report Generator.
    
    The Researcher Agent is responsible for executing RAG queries and collecting
    evidence for each question in a report section. It takes a plan generated by
    the Planner Agent as input and produces evidence for each question.
    """

    def __init__(self, config: ReportGeneratorConfig):
        """Initialize the Researcher Agent.
        
        Args:
            config: Configuration for the Report Generator
        """
        self.config = config
        self.prompt_template = config.researcher_prompt_template
        self.output_dir = Path(config.output_dir) / "researcher"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.rag_path = config.rag_path
        self.rag_pipeline_script = self.rag_path / "main_rag_pipeline.py"
        
        # Validate RAG pipeline script exists
        if not self.rag_pipeline_script.exists():
            raise ValueError(f"RAG pipeline script not found: {self.rag_pipeline_script}")
        
        logger.info(f"Initialized Researcher Agent with output directory: {self.output_dir}")
        logger.info(f"Using RAG pipeline script: {self.rag_pipeline_script}")

    def research_plan(self, plan: Dict[str, Any]) -> Dict[str, Any]:
        """Research the plan by executing RAG queries for each question.
        
        Args:
            plan: The plan generated by the Planner Agent
            
        Returns:
            A dictionary containing the evidence for each question
        """
        logger.info("Researching plan")
        
        # Validate plan
        self._validate_plan(plan)
        
        # Initialize research results
        research_results = {
            "report_title": plan.get("report_title", "Untitled Report"),
            "sections": []
        }
        
        # Process each section in the plan
        for section in plan.get("sections", []):
            section_id = section.get("section_id", "")
            section_title = section.get("section_title", "")
            
            section_research = {
                "section_id": section_id,
                "section_title": section_title,
                "chapter_id": section.get("chapter_id", ""),
                "chapter_title": section.get("chapter_title", ""),
                "description": section.get("description", ""),
                "questions": []
            }
            
            # Process each question in the section
            for question in section.get("questions", []):
                question_id = question.get("qid", "")
                question_text = question.get("text", "")
                queries = question.get("queries", [])
                
                # Execute RAG queries and collect evidence
                question_research = self._research_question(
                    section_id=section_id,
                    question_id=question_id,
                    question_text=question_text,
                    queries=queries
                )
                
                section_research["questions"].append(question_research)
            
            research_results["sections"].append(section_research)
        
        # Save the research results to a file
        self._save_research_results(research_results)
        
        return research_results

    def _validate_plan(self, plan: Dict[str, Any]) -> None:
        """Validate the plan.
        
        Args:
            plan: The plan to validate
            
        Raises:
            ValueError: If the plan is invalid
        """
        if not isinstance(plan, dict):
            raise ValueError("Plan must be a dictionary")
            
        if "sections" not in plan:
            raise ValueError("Plan must contain 'sections' key")
            
        if not isinstance(plan["sections"], list):
            raise ValueError("'sections' must be a list")
            
        for i, section in enumerate(plan["sections"]):
            if not isinstance(section, dict):
                raise ValueError(f"Section {i} must be a dictionary")
                
            if "section_id" not in section:
                raise ValueError(f"Section {i} must have a 'section_id'")
                
            if "section_title" not in section:
                raise ValueError(f"Section {i} must have a 'section_title'")
                
            if "questions" not in section:
                raise ValueError(f"Section {i} must have 'questions'")
                
            if not isinstance(section["questions"], list):
                raise ValueError(f"'questions' in section {i} must be a list")
                
            for j, question in enumerate(section["questions"]):
                if not isinstance(question, dict):
                    raise ValueError(f"Question {j} in section {i} must be a dictionary")
                    
                if "qid" not in question:
                    raise ValueError(f"Question {j} in section {i} must have a 'qid'")
                    
                if "text" not in question:
                    raise ValueError(f"Question {j} in section {i} must have a 'text'")
                    
                if "queries" not in question:
                    raise ValueError(f"Question {j} in section {i} must have 'queries'")
                    
                if not isinstance(question["queries"], list):
                    raise ValueError(f"'queries' in question {j} in section {i} must be a list")

    def _research_question(
        self,
        section_id: str,
        question_id: str,
        question_text: str,
        queries: List[str]
    ) -> Dict[str, Any]:
        """Research a question by executing RAG queries.
        
        Args:
            section_id: ID of the section
            question_id: ID of the question
            question_text: Text of the question
            queries: List of RAG queries to execute
            
        Returns:
            A dictionary containing the evidence for the question
        """
        logger.info(f"Researching question: {question_id} - {question_text}")
        
        question_research = {
            "qid": question_id,
            "text": question_text,
            "evidence": [],
            "consolidated_evidence": ""
        }
        
        # Execute each query and collect evidence
        for i, query in enumerate(queries):
            logger.info(f"Executing query {i+1}/{len(queries)}: {query}")
            
            # Execute RAG query using the RAG pipeline script
            evidence = self._execute_rag_query(query)
            
            query_evidence = {
                "query": query,
                "chunks": evidence
            }
            
            question_research["evidence"].append(query_evidence)
        
        # Consolidate evidence
        consolidated_evidence = self._consolidate_evidence(question_research["evidence"])
        question_research["consolidated_evidence"] = consolidated_evidence
        
        return question_research

    def _execute_rag_query(self, query: str) -> List[Dict[str, Any]]:
        """Execute a RAG query using the RAG pipeline script.
        
        Args:
            query: The query to execute
            
        Returns:
            A list of evidence chunks retrieved by the RAG pipeline
        """
        # Create a unique output directory for this query
        import hashlib
        import time
        
        # Create a unique identifier for this query
        query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
        timestamp = int(time.time())
        query_id = f"{timestamp}_{query_hash}"
        
        # Execute the RAG pipeline script
        try:
            # Prepare the command
            cmd = [
                sys.executable,
                str(self.rag_pipeline_script),
                query,
                "--reranking-enabled",
                "--hybrid-search-enabled",
                "--hybrid-search-weight", "0.7"
            ]
            
            logger.info(f"Executing command: {' '.join(cmd)}")
            
            # Run the command
            result = subprocess.run(
                cmd,
                cwd=str(self.rag_path),
                capture_output=True,
                text=True,
                check=True
            )
            
            logger.info(f"RAG pipeline executed successfully")
            logger.debug(f"RAG pipeline output: {result.stdout}")
            
            # Parse the output to extract evidence chunks
            # In a real implementation, we would parse the output of the RAG pipeline
            # For now, we'll return placeholder evidence chunks
            
            # TODO: Parse actual RAG pipeline output
            evidence_chunks = [
                {
                    "text": f"Evidence chunk 1 for query: {query}",
                    "source": "Document A",
                    "similarity_score": 0.85,
                    "metadata": {"page": 1, "section": "Introduction"}
                },
                {
                    "text": f"Evidence chunk 2 for query: {query}",
                    "source": "Document B",
                    "similarity_score": 0.78,
                    "metadata": {"page": 5, "section": "Analysis"}
                },
                {
                    "text": f"Evidence chunk 3 for query: {query}",
                    "source": "Document C",
                    "similarity_score": 0.72,
                    "metadata": {"page": 12, "section": "Conclusion"}
                }
            ]
            
            return evidence_chunks
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Error executing RAG pipeline: {e}")
            logger.error(f"Stdout: {e.stdout}")
            logger.error(f"Stderr: {e.stderr}")
            return []
        except Exception as e:
            logger.error(f"Error executing RAG query: {e}")
            return []

    def _consolidate_evidence(self, evidence_list: List[Dict[str, Any]]) -> str:
        """Consolidate evidence from multiple queries.
        
        Args:
            evidence_list: List of evidence from multiple queries
            
        Returns:
            Consolidated evidence as a string
        """
        # In a real implementation, this would call an LLM to consolidate the evidence
        # For now, we'll return a simple concatenation of the evidence chunks
        
        consolidated = []
        
        for query_evidence in evidence_list:
            query = query_evidence.get("query", "")
            chunks = query_evidence.get("chunks", [])
            
            consolidated.append(f"Evidence for query: {query}\n")
            
            for chunk in chunks:
                text = chunk.get("text", "")
                source = chunk.get("source", "")
                score = chunk.get("similarity_score", 0.0)
                
                consolidated.append(f"- {text} (Source: {source}, Score: {score:.2f})\n")
            
            consolidated.append("\n")
        
        return "\n".join(consolidated)

    def _save_research_results(self, research_results: Dict[str, Any]) -> None:
        """Save the research results to a file.
        
        Args:
            research_results: The research results to save
        """
        output_file = self.output_dir / "research_results.json"
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(research_results, f, indent=2, ensure_ascii=False)
            
        logger.info(f"Saved research results to {output_file}")